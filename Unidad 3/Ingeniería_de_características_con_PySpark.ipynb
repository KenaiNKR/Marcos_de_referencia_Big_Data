{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Ejercicio 1"
      ],
      "metadata": {
        "id": "ELB0q-sUgueE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install findspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mawkjyN1f9vI",
        "outputId": "1b73b68c-b1cd-4a14-bb2c-bac3ef26a095"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n",
            "Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ww7_doMugAJg",
        "outputId": "2ae29395-a3ec-491b-d02b-40dad89e49ca"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488490 sha256=5c5afb08817eeba3ae87811aa616ff7f500716fe6341432215d5d648e86086ae\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Ag2VbMl4fzj_"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Inicializar la sesión de Spark\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Heart Data Analysis\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType\n",
        "\n",
        "# Definir el esquema del DataFrame\n",
        "schema = StructType([\n",
        "    StructField(\"age\", IntegerType(), True),\n",
        "    StructField(\"sex\", IntegerType(), True),\n",
        "    StructField(\"cp\", IntegerType(), True),\n",
        "    StructField(\"trestbps\", IntegerType(), True),\n",
        "    StructField(\"chol\", IntegerType(), True),\n",
        "    StructField(\"fbs\", IntegerType(), True),\n",
        "    StructField(\"restecg\", IntegerType(), True),\n",
        "    StructField(\"thalach\", IntegerType(), True),\n",
        "    StructField(\"exang\", IntegerType(), True),\n",
        "    StructField(\"oldpeak\", FloatType(), True),\n",
        "    StructField(\"slope\", IntegerType(), True),\n",
        "    StructField(\"ca\", IntegerType(), True),\n",
        "    StructField(\"thal\", IntegerType(), True),\n",
        "    StructField(\"target\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "# Cargar los datos con el esquema definido\n",
        "data_path = '/content/heart.csv'\n",
        "heart_df = spark.read.csv(data_path, header=True, schema=schema)"
      ],
      "metadata": {
        "id": "6G5jqzEKf767"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar las primeras filas del DataFrame\n",
        "heart_df.show(5)\n",
        "\n",
        "# Mostrar el esquema del DataFrame\n",
        "heart_df.printSchema()\n",
        "\n",
        "# Describir las estadísticas básicas de las columnas numéricas\n",
        "heart_df.describe().show()\n",
        "\n",
        "# Contar la distribución de la columna objetivo ('target')\n",
        "heart_df.groupBy('target').count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXuYMXjzgczP",
        "outputId": "703abd09-f7e6-4555-95d7-b435d587fc2c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+------+\n",
            "|age|sex| cp|trestbps|chol|fbs|restecg|thalach|exang|oldpeak|slope| ca|thal|target|\n",
            "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+------+\n",
            "| 63|  1|  3|     145| 233|  1|      0|    150|    0|    2.3|    0|  0|   1|     1|\n",
            "| 37|  1|  2|     130| 250|  0|      1|    187|    0|    3.5|    0|  0|   2|     1|\n",
            "| 41|  0|  1|     130| 204|  0|      0|    172|    0|    1.4|    2|  0|   2|     1|\n",
            "| 56|  1|  1|     120| 236|  0|      1|    178|    0|    0.8|    2|  0|   2|     1|\n",
            "| 57|  0|  0|     120| 354|  0|      1|    163|    1|    0.6|    2|  0|   2|     1|\n",
            "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+------+\n",
            "only showing top 5 rows\n",
            "\n",
            "root\n",
            " |-- age: integer (nullable = true)\n",
            " |-- sex: integer (nullable = true)\n",
            " |-- cp: integer (nullable = true)\n",
            " |-- trestbps: integer (nullable = true)\n",
            " |-- chol: integer (nullable = true)\n",
            " |-- fbs: integer (nullable = true)\n",
            " |-- restecg: integer (nullable = true)\n",
            " |-- thalach: integer (nullable = true)\n",
            " |-- exang: integer (nullable = true)\n",
            " |-- oldpeak: float (nullable = true)\n",
            " |-- slope: integer (nullable = true)\n",
            " |-- ca: integer (nullable = true)\n",
            " |-- thal: integer (nullable = true)\n",
            " |-- target: integer (nullable = true)\n",
            "\n",
            "+-------+------------------+-------------------+------------------+------------------+------------------+-------------------+-----------------+------------------+-------------------+------------------+------------------+------------------+------------------+------------------+\n",
            "|summary|               age|                sex|                cp|          trestbps|              chol|                fbs|          restecg|           thalach|              exang|           oldpeak|             slope|                ca|              thal|            target|\n",
            "+-------+------------------+-------------------+------------------+------------------+------------------+-------------------+-----------------+------------------+-------------------+------------------+------------------+------------------+------------------+------------------+\n",
            "|  count|               303|                303|               303|               303|               303|                303|              303|               303|                303|               303|               303|               303|               303|               303|\n",
            "|   mean|54.366336633663366| 0.6831683168316832| 0.966996699669967|131.62376237623764|246.26402640264027| 0.1485148514851485|0.528052805280528|149.64686468646866|0.32673267326732675|1.0396039587977302|1.3993399339933994|0.7293729372937293|2.3135313531353137|0.5445544554455446|\n",
            "| stddev|  9.08210098983786|0.46601082333962385|1.0320524894832983|  17.5381428135171| 51.83075098793005|0.35619787492797644|0.525859596359298| 22.90516111491409|0.46979446452231655|1.1610750102689427|0.6162261453459622|1.0226063649693276|0.6122765072781408|0.4988347841643915|\n",
            "|    min|                29|                  0|                 0|                94|               126|                  0|                0|                71|                  0|               0.0|                 0|                 0|                 0|                 0|\n",
            "|    max|                77|                  1|                 3|               200|               564|                  1|                2|               202|                  1|               6.2|                 2|                 4|                 3|                 1|\n",
            "+-------+------------------+-------------------+------------------+------------------+------------------+-------------------+-----------------+------------------+-------------------+------------------+------------------+------------------+------------------+------------------+\n",
            "\n",
            "+------+-----+\n",
            "|target|count|\n",
            "+------+-----+\n",
            "|     1|  165|\n",
            "|     0|  138|\n",
            "+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Ejercisio 2"
      ],
      "metadata": {
        "id": "LbgSP6mOgmkB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, when, count, mean\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Verificar los valores nulos en cada columna\n",
        "heart_df.select([count(when(col(c).isNull(), c)).alias(c) for c in heart_df.columns]).show()\n",
        "\n",
        "# Rellenar los valores nulos en características numéricas con la mediana\n",
        "numerical_cols = [c for c, t in heart_df.dtypes if t in ['int', 'double']]\n",
        "for c in numerical_cols:\n",
        "    median = heart_df.approxQuantile(c, [0.5], 0.0)[0]\n",
        "    heart_df = heart_df.fillna({c: median})\n",
        "\n",
        "# Rellenar los valores nulos en características categóricas con el valor más frecuente\n",
        "categorical_cols = [c for c, t in heart_df.dtypes if t == 'string']\n",
        "for c in categorical_cols:\n",
        "    mode = heart_df.groupBy(c).count().orderBy('count', ascending=False).first()[0]\n",
        "    heart_df = heart_df.fillna({c: mode})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whhvPkPDgh4c",
        "outputId": "691e2053-6c8d-4142-a9b4-4d069c8d09c1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+------+\n",
            "|age|sex| cp|trestbps|chol|fbs|restecg|thalach|exang|oldpeak|slope| ca|thal|target|\n",
            "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+------+\n",
            "|  0|  0|  0|       0|   0|  0|      0|      0|    0|      0|    0|  0|   0|     0|\n",
            "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
        "\n",
        "# Ensamblar las características numéricas en un solo vector\n",
        "assembler = VectorAssembler(inputCols=numerical_cols, outputCol=\"features_vector\")\n",
        "heart_df = assembler.transform(heart_df)\n",
        "\n",
        "# Escalar las características\n",
        "scaler = StandardScaler(inputCol=\"features_vector\", outputCol=\"scaled_features\", withStd=True, withMean=True)\n",
        "scaler_model = scaler.fit(heart_df)\n",
        "heart_df = scaler_model.transform(heart_df)"
      ],
      "metadata": {
        "id": "8Y4qKyA1hFi2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "# Codificar características categóricas\n",
        "for c in categorical_cols:\n",
        "    indexer = StringIndexer(inputCol=c, outputCol=c+\"_index\")\n",
        "    heart_df = indexer.fit(heart_df).transform(heart_df)"
      ],
      "metadata": {
        "id": "-xT_9pNIhMPX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Seleccionar las columnas necesarias para el modelo\n",
        "feature_cols = [c+\"_index\" for c in categorical_cols] + [\"scaled_features\"]\n",
        "assembler_final = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "heart_df = assembler_final.transform(heart_df)\n",
        "\n",
        "# Seleccionar las columnas finales\n",
        "heart_df = heart_df.select(\"features\", \"target\")\n",
        "\n",
        "# Mostrar el DataFrame final\n",
        "heart_df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_nazrj8hTCI",
        "outputId": "d5847398-5d68-42f7-91f3-f9715c7f0dee"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+------+\n",
            "|            features|target|\n",
            "+--------------------+------+\n",
            "|[0.95062402146783...|     1|\n",
            "|[-1.9121496945579...|     1|\n",
            "|[-1.4717229690155...|     1|\n",
            "|[0.17987725176857...|     1|\n",
            "|[0.28998393315418...|     1|\n",
            "+--------------------+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MTd5iPa9hv6G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dividir los datos en conjuntos de entrenamiento (80%) y prueba (20%)\n",
        "train_df, test_df = heart_df.randomSplit([0.8, 0.2], seed=42)"
      ],
      "metadata": {
        "id": "RT47NaJ4hXHL"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "# Inicializar el modelo de regresión logística\n",
        "lr = LogisticRegression(labelCol=\"target\", featuresCol=\"features\")\n",
        "\n",
        "# Entrenar el modelo\n",
        "lr_model = lr.fit(train_df)"
      ],
      "metadata": {
        "id": "T-hiGGaUhx28"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Realizar predicciones en el conjunto de datos de prueba\n",
        "predictions = lr_model.transform(test_df)\n",
        "\n",
        "# Mostrar algunas de las predicciones\n",
        "predictions.select(\"features\", \"target\", \"prediction\", \"probability\").show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLBmBv2mh2iV",
        "outputId": "fc76decf-c06e-48eb-d897-f72039e3c1f7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+------+----------+--------------------+\n",
            "|            features|target|prediction|         probability|\n",
            "+--------------------+------+----------+--------------------+\n",
            "|[-2.2424697387148...|     1|       1.0|[1.75031533035104...|\n",
            "|[-2.1323630573292...|     1|       1.0|[2.87577376933252...|\n",
            "|[-1.9121496945579...|     1|       1.0|[5.46417579478814...|\n",
            "|[-1.6919363317867...|     1|       1.0|[2.65467912927529...|\n",
            "|[-1.4717229690155...|     1|       1.0|[1.83349357053014...|\n",
            "+--------------------+------+----------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "# Inicializar el evaluador para el área bajo la curva ROC\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"target\", rawPredictionCol=\"rawPrediction\")\n",
        "\n",
        "# Calcular el área bajo la curva ROC\n",
        "roc_auc = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})\n",
        "\n",
        "# Mostrar el área bajo la curva ROC\n",
        "print(f\"Área bajo la curva ROC: {roc_auc}\")\n",
        "\n",
        "# Calcular la precisión, el recall y el F1-score\n",
        "predictions.groupBy(\"target\", \"prediction\").count().show()\n",
        "\n",
        "# Definir las métricas de evaluación adicionales\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "# Precisión\n",
        "accuracy_evaluator = MulticlassClassificationEvaluator(labelCol=\"target\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = accuracy_evaluator.evaluate(predictions)\n",
        "\n",
        "# Recall\n",
        "recall_evaluator = MulticlassClassificationEvaluator(labelCol=\"target\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
        "recall = recall_evaluator.evaluate(predictions)\n",
        "\n",
        "# F1-score\n",
        "f1_evaluator = MulticlassClassificationEvaluator(labelCol=\"target\", predictionCol=\"prediction\", metricName=\"f1\")\n",
        "f1 = f1_evaluator.evaluate(predictions)\n",
        "\n",
        "# Mostrar las métricas de evaluación\n",
        "print(f\"Precisión: {accuracy}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1-score: {f1}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ZiVMYmGiDxC",
        "outputId": "147dc516-e9e1-4b14-9b42-e9b26ab04da9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Área bajo la curva ROC: 1.0\n",
            "+------+----------+-----+\n",
            "|target|prediction|count|\n",
            "+------+----------+-----+\n",
            "|     0|       0.0|   19|\n",
            "|     1|       1.0|   28|\n",
            "+------+----------+-----+\n",
            "\n",
            "Precisión: 1.0\n",
            "Recall: 1.0\n",
            "F1-score: 1.0\n"
          ]
        }
      ]
    }
  ]
}